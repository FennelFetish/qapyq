# This file contains URLs to prebuilt packages of flash_attn.
# By default, a package for torch 2.8 and CUDA 12.8 is installed.
# If your desired combination of OS, Python, PyTorch and CUDA versions is not listed here,
# check: https://github.com/mjun0812/flash-attention-prebuild-wheels

# To install wheels for a different torch or CUDA version:
#   1. Enable the respective URLs by removing the "#" at the start of the line.
#   2. Disable all other URLs by adding a "#" at the start of the line.


# PyTorch 2.8
# CUDA 12.8

# Linux, FlashAttention 2.8.3
https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3+cu128torch2.8-cp310-cp310-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.10"
https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3+cu128torch2.8-cp311-cp311-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.11"
https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3+cu128torch2.8-cp312-cp312-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.12"
https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.3+cu128torch2.8-cp313-cp313-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.13"

# Windows, FlashAttention 2.8.2
https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.10/flash_attn-2.8.2+cu128torch2.8-cp310-cp310-win_amd64.whl; platform_system == "Windows" and python_version == "3.10"
https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.10/flash_attn-2.8.2+cu128torch2.8-cp311-cp311-win_amd64.whl; platform_system == "Windows" and python_version == "3.11"
https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.10/flash_attn-2.8.2+cu128torch2.8-cp312-cp312-win_amd64.whl; platform_system == "Windows" and python_version == "3.12"
# missing wheel for Python 3.13



# PyTorch 2.8
# CUDA 12.9

# Linux, FlashAttention 2.8.3
# https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3+cu129torch2.8-cp310-cp310-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.10"
# https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3+cu129torch2.8-cp311-cp311-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.11"
# https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3+cu129torch2.8-cp312-cp312-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.12"
# https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.3+cu129torch2.8-cp313-cp313-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.13"



# PyTorch 2.8
# CUDA 12.6

# Linux, FlashAttention 2.8.3
# https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3+cu126torch2.8-cp310-cp310-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.10"
# https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3+cu126torch2.8-cp311-cp311-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.11"
# https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3+cu126torch2.8-cp312-cp312-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.12"
# https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.3+cu126torch2.8-cp313-cp313-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.13"

# Windows, FlashAttention 2.8.2
# https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.2+cu126torch2.8-cp313-cp313-win_amd64.whl; platform_system == "Windows" and python_version == "3.13"



# PyTorch 2.9
# CUDA 12.8

# Linux, FlashAttention 2.8.3
#https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.8.3+cu128torch2.9-cp310-cp310-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.10"
#https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.8.3+cu128torch2.9-cp311-cp311-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.11"
#https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.8.3+cu128torch2.9-cp312-cp312-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.12"
#https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.8.3+cu128torch2.9-cp313-cp313-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.13"
