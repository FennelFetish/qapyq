# PyTorch 2.8
# CUDA 12.8

# Linux, FlashAttention 2.8.3
https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3+cu128torch2.8-cp310-cp310-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.10"
https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3+cu128torch2.8-cp311-cp311-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.11"
https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3+cu128torch2.8-cp312-cp312-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.12"
https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.3+cu128torch2.8-cp313-cp313-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.13"

# Windows, FlashAttention 2.8.2
https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.10/flash_attn-2.8.2+cu128torch2.8-cp310-cp310-win_amd64.whl; platform_system == "Windows" and python_version == "3.10"
https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.10/flash_attn-2.8.2+cu128torch2.8-cp311-cp311-win_amd64.whl; platform_system == "Windows" and python_version == "3.11"
https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.10/flash_attn-2.8.2+cu128torch2.8-cp312-cp312-win_amd64.whl; platform_system == "Windows" and python_version == "3.12"
# missing wheel for Python 3.13



# PyTorch 2.8
# CUDA 12.9

# Linux, FlashAttention 2.8.3
# https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3+cu129torch2.8-cp310-cp310-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.10"
# https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3+cu129torch2.8-cp311-cp311-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.11"
# https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3+cu129torch2.8-cp312-cp312-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.12"
# https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.3+cu129torch2.8-cp313-cp313-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.13"



# PyTorch 2.8
# CUDA 12.6

# Linux, FlashAttention 2.8.3
# https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3+cu126torch2.8-cp310-cp310-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.10"
# https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3+cu126torch2.8-cp311-cp311-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.11"
# https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3+cu126torch2.8-cp312-cp312-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.12"
# https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.3+cu126torch2.8-cp313-cp313-linux_x86_64.whl; platform_system == "Linux" and python_version == "3.13"

# Windows, FlashAttention 2.8.2
# https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.2+cu126torch2.8-cp313-cp313-win_amd64.whl; platform_system == "Windows" and python_version == "3.13"
